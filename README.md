
# CHIRPBIRD SONG RECOGNITION USING DEEP LEARNING
This was a winter project offered by GDSC, IITK which involved usage of neural networks to classify the audios of different categories of birds.The issue tackled by this project is to automate the remote monitoring of bird populations. A detailed description is given below regarding this project.



## DATA COLLECTION

We used the [`Cornell Birdcall Identification`](https://www.kaggle.com/competitions/birdsong-recognition) dataset which was availabel on Kaggle. Some basic information about the dataset are:
- It was a dataset which comprised of around 264 unique bird codes and 213755 bird audios.
- The train data consisted of short recordings of individual bird calls generously uploaded by users of [`xeno-canto.org`](https://xeno-canto.org/).

## DATA PREPROCESSING
The data had a large number of audio files and along with that many other features like elevation, longitude, latitude, bird code, species and many more. Other than the features which were given to us in the dataset, we also extracted the features from the audios of the birds. 
To analyze an audio, there are many parameters. Some of them are:
- **MFCC (Mel-Frequency Cepstral Coefficients):** A.k.a ‘Most-frequently considered coefficients’, MFCC is that one feature you would see being used in any machine learning experiment involving audio files.Any sound generated by humans is determined by the shape of their vocal tract (including tongue, teeth, etc). If this shape can be determined correctly, any sound produced can be accurately represented. The envelope of the time power spectrum of the speech signal is representative of the vocal tract and MFCC (which is nothing but the coefficients that make up the Mel-frequency cepstrum) accurately represents this envelope.
  Generally the first 13 coefficients(the lower dimensions) of MFCC are taken as features as they represent the envelope of spectra. And the discarded higher dimensions expresses the spectral details. For different phonemes, envelopes are enough to represent the difference, so we can recognize phonemes through MFCC.

- **Zero-crossing rate:** Zero-crossing rate is a measure of number of times in a given time interval/frame that the amplitude of the speech signals passes through a value of zero. This feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.

There are many more features of an audio but the major ones have been covered up above. For further detail please visit: [`Features of Audio`](https://towardsdatascience.com/how-i-understood-what-features-to-consider-while-training-audio-files-eedfb6e9002b)



